---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

# **About Me**

Yidi Li is currently an _Associate Professor_ in the College of Computer Science and Technology at Taiyuan University of Technology (å¤ªåŸç†å·¥å¤§å­¦, è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢), and the director of the _Multimodal Intelligent Human-Robot Interaction Laboratory (***[å¤šæ¨¡æ€æ™ºèƒ½äººæœºäº¤äº’å®éªŒå®¤]([https://robotics.pkusz.edu.cn](https://lab.rjmart.cn/11097/MIHRI))***, MIHRI Lab)_. Prior to that, she received the PhD degree in Computer Science and Technology from Peking University under the supervision of ***[Prof. Hong Liu](https://robotics.pkusz.edu.cn)*** in 2023. She has published more than 20 papers in top international conferences and SCI journals in the field of artificial intelligence. She serves as a reviewer for many important journals and conferences in the field of artificial intelligence, such as CAAI TRIT, AAAI, and ICASSP, etc. She serves as an expert judge for 3D Digital Innovation Design Competition, and other national events.

Yidi Li's research work centers on audio-visual fusion, and is dedicated to improving the model's adaptability and robustness in complex dynamic scenarios by investigating multimodal heterogeneous data fusion problems, and multimodal multilevel information interaction problems. Her research interests include audio-visual learning, sound source localization, speech recognition, and speaker tracking.

# ğŸ“£ğŸ“£ Call for membersğŸ“£ğŸ“£
<span style="color:red;"><strong>å¤šæ¨¡æ€æ™ºèƒ½äººæœºäº¤äº’å®éªŒå®¤(MIHRI Lab)ç°æ­£æ‹›æ”¶2025ã€2026å¹´å…¥å­¦çš„ç ”ç©¶ç”Ÿï¼Œå¤§ä¸€/å¤§äºŒä¼˜ç§€æœ¬ç§‘ç”Ÿï¼</strong></span>

<font color=red>æˆ‘ä»¬å¯»æ±‚å¯¹äººå·¥æ™ºèƒ½ã€æœºå™¨äººæŠ€æœ¯ã€è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸå……æ»¡çƒ­æƒ…çš„ä¼˜ç§€ä¿ç ”/è€ƒç ”å­¦ç”Ÿã€‚æ¬¢è¿ç¼–ç¨‹èƒ½åŠ›è¾ƒå¥½ã€æœ‰æ·±åº¦å­¦ä¹ å®è·µç»éªŒã€ç¨‹åºè®¾è®¡ç«èµ›æˆ–è€…ç§‘ç ”ç»å†ï¼Œæœ‰å¿—äºæ”»è¯»ç¡•å£«/åšå£«ç ”ç©¶ç”Ÿå’Œå‡ºå›½æ·±é€ çš„åŒå­¦ä¸æˆ‘è”ç³»ï¼ˆliyidi@tyut.edu.cnï¼‰ï¼Œä¹Ÿæ¬¢è¿å¤§ä¸€/å¤§äºŒçš„ä¼˜ç§€æœ¬ç§‘ç”Ÿè¿›ç»„å­¦ä¹ ã€‚</font><br>

<span style="color:red;"><strong>MIHRI Labå°†ä¸ºæˆå‘˜æä¾›ï¼š</strong></span><br>

- <font color=red>å‰æ²¿ç ”ç©¶ï¼šå‚ä¸ä¸°å¯Œçš„å‰æ²¿ç ”ç©¶è¯¾é¢˜ã€‚</font>
- <font color=red>å­¦æœ¯äº¤æµï¼šå‚åŠ å›½å†…/å›½é™…ä¼šè®®ï¼Œæ‰©å±•å­¦æœ¯è§†é‡ã€‚</font>
- <font color=red>å›½é™…åˆä½œï¼šæµ·å¤–åæ ¡åˆä½œä¸“å®¶è”åˆæŒ‡å¯¼ã€‚</font>
- <font color=red>å­¦ä¹ è®¿é—®ï¼šä¼˜ç§€å­¦ç”Ÿå¯æ¨èè‡³å›½å†…å¤–çŸ¥åé™¢æ ¡å­¦ä¹ è®¿é—®ã€‚</font>
  
# ğŸ¥³ News
- 2024.12ğŸ“„One ICASSP paper accepted!
- 2024.12ğŸ“„Two CAAI papers accepted!
- 2024.11ğŸ†Dr. Li awarded IEEE Outstanding Service Award at the 2024 IEEE Smart World Congress!
- 2024.9 ğŸ“„One TMM paper accepted! (SCI Q1 TOP)
- 2024.8 ğŸŒŸYoung Scientists Fund of the National Natural Science Foundation of China approved!
- 2024.8 ğŸ§‘â€ğŸ¤â€ğŸ§‘Successful completion of the MIHRI Lab Summer Training for outstanding undergraduate students!
- 2024.8 ğŸ“„One CWSN paper accepted!
- 2024.7 ğŸ†Dr. Li awarded the 2024 ACM Rising Star Award (Taiyuan)!
- 2024.7 ğŸ“„One BIBM paper accepted!
- 2024.7 ğŸŒŸInnovation Project for Higher Education Institutions in Shanxi Province approved!
- 2024.7 ğŸ“„One ACAIT paper accepted!
- 2024.5 ğŸŒŸYouth Project of the Shanxi Province Basic Research Program approved!
- 2024.4 ğŸ“„One TMM paper accepted!

<!-- 2024.8 å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘é’å¹´é¡¹ç›®è·æ‰¹ï¼
2024.8 MIHRI Labä¼˜ç§€æœ¬ç§‘ç”Ÿæš‘æœŸé›†è®­åœ†æ»¡ç»“æŸï¼
2024.8 ä¸€ç¯‡CSWCè®ºæ–‡è¢«æ¥æ”¶ï¼
2024.7 Dr.Liè£è·2024ACMæ–°æ˜Ÿå¥–ï¼ˆå¤ªåŸåˆ†ä¼šï¼‰
2024.7 ä¸€ç¯‡BIBMè®ºæ–‡è¢«æ¥æ”¶ï¼
2024.7 å±±è¥¿çœé«˜ç­‰å­¦æ ¡ç§‘æŠ€åˆ›æ–°é¡¹ç›®è·æ‰¹ï¼
2024.7 ä¸€ç¯‡ACAITè®ºæ–‡è¢«æ¥æ”¶ï¼
2024.5 å±±è¥¿çœåŸºç¡€ç ”ç©¶è®¡åˆ’é’å¹´é¡¹ç›®è·æ‰¹ï¼
2024.4 ä¸€ç¯‡TMMè®ºæ–‡è¢«æ¥æ”¶ï¼
2024.3 Dr.Liè¢«é€‰ä¸ºä¸­å›½ç§‘åâ€œé’å¹´ç§‘å­¦å®¶ç™¾åŸè¡Œâ€å±±è¥¿ä»£è¡¨ï¼
2023.12 ä¸¤ç¯‡ICASSPè®ºæ–‡è¢«æ¥æ”¶-->

# ğŸ“œ Research Area
- **Multi-modal Learning:** 

Speaker tracking, Sound source localization, Speech recognition, Audio-visual event localization, Emotion recognition

- **Computer Vision:**

Industrial vision, Action recognition, Object tracking


# ğŸ’» Research Experiences
- 2013.09 - 2017.07: B.Sc. in Statistics, Taiyuan University of Technology, China
- 2017.09 - 2023.07: Ph.D. in Computer Science, Peking University, China
- 2023.7 - Present: Associate Professor, Taiyuan University of Technology, China


# ğŸ“ Publications 
- [20] **Yidi Li**, Hong Liu, Bing Yang. STNet: Deep Audio-Visual Fusion Network for Robust Speaker Tracking. IEEE Transactions on Multimedia (**TMM**), 2024. <font color=red>(SCI Q1-top)</font>
- [19]	**Yidi Li**, Wenkai Zhao, Zeyu Wang, Zhenhuan Xu, Bin Ren, Nicu Sebe. Multi-Stage Multimodal Distillation for Audio-Visual Speaker Tracking, Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024.
- [18]	Jie Xiang, Ang Zhao, Xia Li, Xubin Wu, Yanqing Dong, Yan Niu, **Yidi Li**, Xin Wen, Enhancing Brain MRI Super-Resolution through Multi-Slice Aware Matching and Fusion, CAAI Transactions on Intelligence Technology, 2024.
- [17] Zihao Mi, Xueyu Liu, Jianan Zhang, Guangze Shi, **Yidi Li**, and Yongfei Wu. Multi-instance Curriculum Learning for Histopathology Image Classifications with Hard Negative Mining and Positive Augmentation. **BIBM**, 2024.<font color=red> (CCF B)</font>
- [16] Linhui Sun, **Yidi Li**, Xujiao Zhao, Kaiyi Wang, Hao Guo*. Event-RGB Fusion for insulator Defect Detection Based on lmproved YOLOv8. ACAIT, 2024.
- [15] **Yidi Li**, Kairan Zhang, Chenxu Yang, Sizhou Liu, Hao Guo, Hongfei Zhang. A Synthesis Library Subset Screening Method for High Energy Efficiency Requirements. CWSN, 2024.
- [14] Tao Wang, Mengyuan Liu, Hong Liu, Wenhao Li, Miaoju Ban, Tianyu Guo and **Yidi Li**. Feature Completion Transformer for Occluded Person Re-identification. IEEE Transactions on Multimedia (**TMM**), 2024. <font color=red>(SCI Q1-top)</font>
- [13] Zhenhuan Xu, Yongfei Wu, Liming Zhang, **Yidi Li**, Adaptive Fourier Decomposition Based Signal Extraction on Weak Electromagnetic Field, Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (**ICASSP**), 2024. <font color=red>(CCF B)</font>
- [12] Ruijia Fan, Hong Liu, **Yidi Li**, ATTA-NET: Attention Aggregation Network for Audio-Visual Emotion Recognition, Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (**ICASSP**), 2024. <font color=red>(CCF B)</font>
- [11] **Yidi Li**, Guoquan Wang, Zhan Chen, Hao Tang, and Hong Liu. On-Device Audio-Visual Multi-Person Wake Word Spotting, CAAI Transactions on Intelligence Technology (**CAAI TRIT**), 2023. <font color=red>(JCR Q1)</font>
- [10] **Yidi Li**, Jiale Ren, Yawei Wang, Xia Li, and Hong Liu. Audio-Visual Keyword Transformer for Unconstrained Sentence-Level Keyword Spotting, CAAI Transactions on Intelligence Technology (**CAAI TRIT**), 2023. <font color=red>(JCR Q1)</font>
- [9] Guoquan Wang, Hong Liu, Tianyu Guo, Jingwen Guo, Ti Wang, **Yidi Li**. Self-supervised 3D Skeleton Representation Learning with Active Sampling and Adaptive Relabeling for Action Recognition. Proceedings of IEEE International Conference on Image Processing (**ICIP**), 2023. 
- [8] Xingyue Shi, Hong Liu, Wei Shi, Zihui Zhou, **Yidi Li**. Boosting Person Re-Identification with Viewpoint Contrastive Learning and Adversarial Training. Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (**ICASSP**), 2023. <font color=red>(CCF B)</font>
- [7] Wanruo Zhang, Hong Liu, Jianbing Wu, **Yidi Li**. MVSSC: Meta Reinforcement Learning Based Visual lndoor Navication Using Multi-view Semantic Spatial Context, Pattern Recognition Letters (**PRL**). 2023. <font color=red>(CAAI B)</font>
- [6] Jian Zhang, Ge Yang, Runwei Ding, **Yidi Li**. Cascade RDN: Towards Accurate Localization in Industrial Visual Anomaly Detection with Structural Anomaly Generation, IEEE Robotics and Automation Letters (**RAL**), 2023. <font color=red>(CAAI B)</font>
- [5] **Yidi Li**, Hong Liu, Hao Tang. Multi-Modal Perception Attention Network with Self-Supervised Learning for Audio-Visual Speaker Tracking, Proceedings of the AAAI Conference on Artificial Intelligence (**AAAI**) <font color=red>(Oral)</font>. 2022. <font color=red>(CCF A)</font>
- [4] Peini Guo, Zhengyan Chen, **Yidi Li**, and Hong Liu. Audio-Visual Fusion Network Based on Conformer for Multimodal Emotion Recognition. Artificial Intelligence (**CICAI**), 2022. <font color=red>(CAAI A)</font>
- [3] Hong Liu, Yongheng Sun, **Yidi Li**, Bing Yang. 3D Audio-Visual Speaker Tracking with a Novel Particle Filter. Proceedings of International Conference on Pattern Recognition (**ICPR**), 2021.
- [2] **Yidi Li**, Hong Liu, Bing Yang, Runwei Ding, Yang Chen. Deep Metric Learning-Assisted 3D Audio-Visual Speaker Tracking via Two-Layer Particle Filter. **Complexity**, 2020.
- [1] Hong Liu, **Yidi Li**, Bing Yang. 3D Audio-Visual Speaker Tracking with a Two-Layer Particle Filter. Proceedings of IEEE International Conference on Image Processing (**ICIP**), 2019. 

# ğŸŒŸ Projects
- [1] Young Scientists Fund of the National Natural Science Foundation of China, 2024.
- [2] Scientific and Technologial Innovation Programs of Higher Education Institutions in Shanxi, 2024.
- [3] Shanxi Provincial Department of Science and Technology Basic Research Project, 2024.

# ğŸ… Certifications and Awards
- 2024 <font color=red>ACM Rising Star Award</font>. (Taiyuan)
- The 16th National 3D Digital Innovation Design Competition å…¨å›½ä¸‰ç»´æ•°å­—åŒ–åˆ›æ–°è®¾è®¡å¤§èµ›ï¼ˆAç±»å­¦ç§‘ç«èµ›ï¼‰, 2023, <font color=red>National First Prize</font>. (Competition Supervisor)
- The 20th National College Student Information Security and Adversarial Technology Competition å…¨å›½å¤§å­¦ç”Ÿä¿¡æ¯å®‰å…¨ä¸å¯¹æŠ—æŠ€æœ¯ç«èµ›ï¼ˆAç±»å­¦ç§‘ç«èµ›ï¼‰, 2023, <font color=red>National First Prize</font>. (Competition Supervisor)

<!-- Google Analytics -->
<script async src="https://catherine-qian.github.io/"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'GA_MEASUREMENT_ID');
</script>
<!-- End Google Analytics -->



